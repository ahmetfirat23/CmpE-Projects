<!doctype html>
<html lang="en">
<head>
    <title>CMPE591: Deep Learning in Robotics</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <link href="css/styles.css" rel="stylesheet">
</head>

<body>
    <div class="w3-container w3-margin-bottom" style="max-width: 960px; margin: auto">
        <header class="">
            <h1 class="w3-center">CMPE591: Deep Learning in Robotics</h1>
            <nav class="w3-bar">
                <a class="w3-bar-item w3-button" href="index.html">Syllabus</a>
                <a class="w3-bar-item w3-button" href="homeworks.html">Homeworks</a>
            </nav>
        </header>
        <details>
            <summary><h2>Preparing the Environment</h2></summary>
            <div class="w3-row">
                It is suggested that you install a virtual environment for this course. You can use Anaconda or Miniconda (smaller size) for this purpose. You can download Anaconda from <a href="https://www.anaconda.com/products/distribution">here</a>. Alternatively, you can use Mamba (a faster version of conda) for this purpose. You can download Mamba from <a href="https://github.com/conda-forge/miniforge#mambaforge">here</a>. Install the downloaded script by running the following command in your terminal:
                <pre>$ bash &ltdownloaded_script&gt.sh</pre>
                After the installation, you can create a virtual environment by running the following command:
<pre>
# For Anaconda
$ conda create -n &ltvirtual_environment_name&gt python=3.9
$ conda activate &ltvirtual_environment_name&gt

# For Mamba
$ mamba create -n &ltvirtual_environment_name&gt python=3.9
$ mamba activate &ltvirtual_environment_name&gt
</pre>
                You will need to run <code>mamba activate &ltvirtual_environment_name&gt</code> (or <code>conda</code>) every time you open a new terminal to activate the virtual environment. You can deactivate the virtual environment by running <code>mamba deactivate</code>.<br><br>
                We will use <a href="https://mujoco.org">MuJoCo</a> and <a href="https://github.com/deepmind/dm_control">dm_control</a> for our simulation environment. You can install them by running:
<pre>
$ pip install dm_control  # dm_control automatically installs mujoco
</pre>
                also install some other dependencies:
<pre>
$ pip install git+https://github.com/alper111/mujoco-python-viewer.git
$ pip install pyyaml
$ mamba install numpy  # or conda install numpy
</pre>
                Additionally, we will use PyTorch for training our models, check out the <a href="https://pytorch.org/get-started/locally/">installation instructions</a>. After installing PyTorch, clone the homework repository by running:
<pre>
$ git clone https://github.com/cmpe591/cmpe591.github.io.git
</pre>
Homeworks will be released in the <code>src</code> folder of the repository. You can run the demo code by running:
<pre>
$ cd cmpe591.github.io/src
$ python demo.py
</pre>
                You should see the following output:
                <img class="w3-margin-top w3-margin-bottom" src="images/hw1.png" alt="homework1" style="width: 100%">
                <code>environment.py</code> and <code>mujoco_menagerie</code> will be common throughout homeworks and <code>homework&ltx&gt.py</code> will be added each week.
    It is suggested that you use <a href="https://code.visualstudio.com/">Visual Studio Code</a> with GitHub Copilot for easier development (though double-check everything that copilot suggests). GitHub Copilot is free for students.
            </div>
        </details>
        <details>
            <summary><h2>Homework 1 (Training a DNN using PyTorch)</h2></summary>
            <div class="w3-row">
                <video autoplay loop style="border-radius: 5px;">
                    <source src="images/hw1gif.mp4" type="video/mp4">
                  </video>
            </div>
            <div class="w3-row">
                In this homework, you will train a deep neural network that estimates the object's position given the executed action and the state (a top-down view of the environment). Below are some example states.
                <div class="w3-container w3-center w3-margin-top w3-margin-bottom">
                <img src="images/hw1_states.png" alt="homework1" style="width: 80%">
                </div>
                There are two object types (cube and sphere) with random sizes between 2cm to 3cm, and the robot randomly pushes the object to four main directions. Based on the object's type and size, the resulting object position changes. Assuming that you have already cloned the repository, you can run the following code for sampling the data:
<pre>
import numpy as np
from homework1 import Hw1Env

env = Hw1Env(render_mode="gui")
for _ in range(100):
    env.reset()
    action_id = np.random.randint(4)
    _, img_before = env.state()
    env.step(action_id)
    pos_after, img_after = env.state()
    env.reset()
</pre>
                You might also want to check the main part of the <code>homework1.py</code> to see how to collect data with multiple processes. Sample 1000 data point and train
                <ol>
                    <li>A plain multi-layer perceptron (MLP)</li>
                    <li>Convolutional neural network</li>
                </ol>
                using PyTorch.
                <h3>Optional</h3>
                Instead of predicting the object's position, predict the raw image of the environment after the action.
            </div>
        </details>
        <details>
            <summary><h2>Homework 2 (Deep Q Network)</h2></summary>
            <div class="w3-row w3-center">
                <video autoplay loop style="border-radius: 5px;">
                    <source src="images/hw2gif.mp4" type="video/mp4">
                  </video>
            </div>
            <div class="w3-row">
                In this homework, you will train a deep Q network (DQN) that learns to push the object to the desired position. There has been a couple of updates in the environment file, so make sure to pull the latest version of the repository by running <code>git pull</code>. You can run the following code to interact with the environment (also see <code>homework2.py</code> after pulling the latest version):
<pre>
import numpy as np

from homework2 import Hw2Env

N_ACTIONS = 8
env = Hw2Env(n_actions=N_ACTIONS, render_mode="gui")
for episode in range(10):
    env.reset()
    done = False
    cum_reward = 0.0
    while not done:
        action = np.random.randint(N_ACTIONS)
        state, reward, is_terminal, is_truncated = env.step(action)
        done = is_terminal or is_truncated
        cum_reward += reward
    print(f"Episode={episode}, reward={cum_reward}")
</pre>
                If you want to work on a remote machine with no screen, make sure you set the following environment variables:
<pre>
export MUJOCO_GL=egl
export PYOPENGL_PLATFORM=egl
</pre>
                The reward is set to the following <var>1/distance(ee, obj)+1/distance(obj, goal)</var> where <var>ee</var> is the end-effector position, <var>obj</var> is the object position, and <var>goal</var> is the goal position. Tuning with hyperparameters can be tricky, so you can use the following hyperparameters:
<pre>
Network(
    Conv2d(3, 32, 4, 2, 1), ReLU(),  # (-1, 3, 128, 128) -> (-1, 32, 64, 64)
    Conv2d(32, 64, 4, 2, 1), ReLU(),  # (-1, 32, 64, 64) -> (-1, 64, 32, 32)
    Conv2d(64, 128, 4, 2, 1), ReLU(),  # (-1, 64, 32, 32) -> (-1, 128, 16, 16)
    Conv2d(128, 256, 4, 2, 1), ReLU(),  # (-1, 128, 16, 16) -> (-1, 256, 8, 8)
    Conv2d(256, 512, 4, 2, 1), ReLU(),  # (-1, 256, 8, 8) -> (-1, 512, 4, 4)
    Avg([2, 3])  # average pooling over the spatial dimensions  (-1, 512, 4, 4) -> (-1, 512),
    Linear(512, N_ACTIONS)
)
GAMMA = 0.99
EPSILON = 1.0
EPSILON_DECAY = 0.999 # decay epsilon by 0.999 every EPSILON_DECAY_ITER
EPSILON_DECAY_ITER = 10 # decay epsilon every 100 updates
MIN_EPSILON = 0.1 # minimum epsilon
LEARNING_RATE = 0.0001
BATCH_SIZE = 32
UPDATE_FREQ = 4 # update the network every 4 steps
TARGET_NETWORK_UPDATE_FREQ = 100 # update the target network every 1000 steps
BUFFER_LENGTH = 10000
</pre>
                This set is not definitive, but it seems to converge to a good policy. Feel free to share your good set of hyperparameters with the class. Plot the reward over episodes and add it to your submission. You can use <code>high_level_state</code> to get a higher-level state, instead of raw pixels. This might speed up your experimentation as you would not need to train a convolutional network.
        </details>
        <details>
            <summary><h2>Homework 3 (Policy Gradient Methods)</h2></summary>
            <div class="w3-row">
                In this homework, you will train either a vanilla policy gradient or proximal policy optimization (PPO) model to learn to push the object to the desired position. The environment is the same as the previous homework, except the actions are now continuous. Some boilerplate code is provided in <code>homework3.py</code> to collect data in multiple processes. Note that there might be more efficient implementations, and you are absolutely free not to use the provided code. In fact, you are free to use high-level implementations such as [Stable Baselines](https://stable-baselines.readthedocs.io/en/master/). However, you will need to define the environment yourself (which is not very hard). Make sure to pull the latest version of the repository by running <code>git pull</code>. Stable hyperparameters will be shared soon. Feel free to share your good set of hyperparameters with the class. Plot the reward over episodes and add it to your submission. As in HW2, you can use <code>high_level_state</code> to get a higher-level state instead of raw pixels.
            </div>
        </details>
        <details>
            <summary><h2>Homework 4 (Offline RL)</h2></summary>
            <div class="w3-row">
                In one short paragraph, please explain the advantages of offline RL in [1] over online RL in this particular work where data was not obtained from external reasources but was collected by the researchers themselves.
                <br>
                <br>
                [1] Kalashnikov D, Irpan A, Pastor P, Ibarz J, Herzog A, Jang E, Quillen D, Holly E, Kalakrishnan M, Vanhoucke V, Levine S. Scalable deep reinforcement learning for vision-based robotic manipulation. InConference on Robot Learning 2018 Oct 23 (pp. 651-673). PMLR.
            </div>
        </details>
        <details open>
            <summary><h2>Homework 5 (Learning from Demonstration with CNMPs)</h2></summary>
            <div class="w3-row w3-center">
                <video autoplay loop style="border-radius: 5px;">
                    <source src="images/hw5gif.mp4" type="video/mp4">
                  </video>
            </div>
            <div class="w3-row">
                In this homework, you will collect demonstrations that consist of (<i>t</i>, <i>e<sub>y</sub></i>, <i>e<sub>z</sub></i>, <i>o<sub>y</sub></i>, <i>o<sub>z</sub></i>) where <i>e</i> and <i>o</i> are the end-effector and the object cartesian coordinates with subscripts denoting the relevant axis. The code for collecting demonstrations is provided in <code>homework5.py</code>. The robot randomly moves its end-effector in the y-z plane, sometimes hitting the object and sometimes not. The height of the object is random and provided from the environment as well. You will train a CNMP with the following dataset: {(<i>t</i>, <i>e<sub>y</sub></i>, <i>e<sub>z</sub></i>, <i>o<sub>y</sub></i>, <i>o<sub>z</sub></i>)<sub>i</sub>, <i>h</i><sub>i</sub>}<sup>N</sup><sub>i=0</sub> where <i>h</i> is the height of the object. Here, <i>t</i> will be the query dimension, <i>h</i> will be the condition to be given to the decoder, and other dimensions will be target dimensions while training the CNMP. In other words, given several context points (with all dimensions provided), the model will be asked to predict the end-effector and the object positions given the time and the height of the object.
            </div>
        </details>
    </div>
</body>

</html>